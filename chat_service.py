from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse, JSONResponse
from relevanceai import RelevanceAI, APIError
import os
import uuid

"""
FastAPI micro-service that provides a single /chat endpoint.
It first attempts to stream responses from Relevance AI via `agents.chat_stream`.
If streaming is disabled or not supported for the given agent, it falls back to
triggering a task + polling until completion.

Environment variables required:
- RAI_AUTH_TOKEN   (project_id:api_key)
- RAI_REGION       (e.g. d7b62b)
- AGENT_ID         (default agent when not provided by client)
"""

client = RelevanceAI()  # Automatically picks up credentials from env

app = FastAPI()


@app.post("/chat")
async def chat(req: Request):
    """Primary chat endpoint used by Node/Socket.IO backend.
    Expected JSON body:
        {
          "message": "user's prompt",
          "agent_id": "optional-agent-id",    # falls back to env AGENT_ID
          "thread_id": "optional-thread-id",  # autogenerated if missing
          "stream": true                       # default true
        }
    """
    data = await req.json()
    agent_id: str = data.get("agent_id") or os.getenv("AGENT_ID")
    if not agent_id:
        return JSONResponse({"error": "agent_id must be provided through body or env AGENT_ID"}, status_code=400)

    message: str = data.get("message")
    if not message:
        return JSONResponse({"error": "message field is required"}, status_code=400)

    thread_id: str = data.get("thread_id") or f"thread_{uuid.uuid4().hex}"
    stream: bool = bool(data.get("stream", True))

    # Attempt streaming first
    if stream:
        try:
            def gen():
                try:
                    for part in client.agents.chat_stream(
                        agent_id=agent_id,
                        message=message,
                        thread_id=thread_id,
                    ):
                        # Each part is assumed to be a dict with "content" key
                        chunk = part.get("content")
                        if chunk is not None:
                            yield f"data:{chunk}\n\n"
                except Exception as inner_err:
                    # In case the generator errors out mid-stream, send error then stop
                    yield f"data:[STREAM_ERROR] {str(inner_err)}\n\n"
            return StreamingResponse(gen(), media_type="text/event-stream")
        except APIError as e:
            # Fallback only for the explicit streaming-disabled scenario
            if "stream is not enabled" not in str(e).lower():
                raise
            # Otherwise, fall back to polling
            stream = False
        except Exception:
            # Unknown error – force fallback
            stream = False

    # Fallback path → trigger task + polling
    try:
        task = client.agents.trigger_task(
            agent_id=agent_id,
            message=message,
            thread_id=thread_id,
        )
    except Exception as e:
        return JSONResponse({"error": f"Failed to trigger task: {str(e)}"}, status_code=500)

    try:
        output = client.tasks.wait_for_completion(
            task.conversation_id,
            timeout=int(os.getenv("POLL_TIMEOUT", 30)),   # seconds configurable via env
        )
    except Exception as e:
        return JSONResponse({"error": f"Polling failed: {str(e)}"}, status_code=500)

    # Make a best-effort to extract the assistant message from possible fields
    response_text = (
        (getattr(output, "output", {}) or {}).get("message")
        or getattr(output, "response", None)
        or getattr(output, "task", {}).get("response")
        or ""
    )

    return JSONResponse({
        "thread_id": thread_id,
        "response": response_text,
    })
