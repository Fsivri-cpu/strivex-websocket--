# Gerekli patching işlemlerini yapacak modülü en önce import edelim
import sys
import urllib.parse
import os
import uuid
import traceback

# urllib.parse modülündeki eksik sabit hatasını çözelim
if not hasattr(urllib.parse, 'MAX_CACHE_SIZE'):
    urllib.parse.MAX_CACHE_SIZE = 20

# tkinter'ı mock edelim
class MockTkinter:
    class Label:
        pass
    
    # Diğer gerekli tkinter sınıfları    
    class Tk:
        def __init__(self):
            pass
        def mainloop(self):
            pass
        def destroy(self):
            pass

# tkinter modülünü patching yapalım
sys.modules['tkinter'] = MockTkinter()

# Şimdi standart FastAPI importlarını yapalım
from fastapi import FastAPI, Request
from fastapi.responses import StreamingResponse, JSONResponse

# Şimdi RelevanceAI'yı import edelim
try:
    from relevanceai import RelevanceAI
    # exceptions modülü import etme denemeleri
    try:
        from relevanceai.exceptions import APIError
    except ImportError:
        try:
            from relevanceai import APIError
        except ImportError:
            APIError = Exception
except Exception as e:
    print(f"RelevanceAI import hatası: {str(e)}")
    print("Detaylı hata bilgisi:")
    traceback.print_exc()
    # Hata durumunda program çalışmaya devam etsin, böylece hata loglarını görebiliriz
    raise

"""
FastAPI micro-service that provides a single /chat endpoint.
It first attempts to stream responses from Relevance AI via `agents.chat_stream`.
If streaming is disabled or not supported for the given agent, it falls back to
triggering a task + polling until completion.

Environment variables required:
- RELEVANCEAI_API_KEY  (your API key)
- RELEVANCEAI_PROJECT  (your project ID)
- RELEVANCEAI_REGION   (e.g. d7b62b)
- AGENT_ID             (default agent when not provided by client)
"""

# Çevre değişkenlerini kontrol edelim ve loglayalım
print(f"[ENV] RELEVANCEAI_API_KEY set: {'Yes' if os.getenv('RELEVANCEAI_API_KEY') else 'No'}")
print(f"[ENV] RELEVANCEAI_PROJECT set: {'Yes' if os.getenv('RELEVANCEAI_PROJECT') else 'No'}")
print(f"[ENV] RELEVANCEAI_REGION set: {'Yes' if os.getenv('RELEVANCEAI_REGION') else 'No'}")
print(f"[ENV] AGENT_ID set: {'Yes' if os.getenv('AGENT_ID') else 'No'}")

# RelevanceAI client'ı başlat
client = RelevanceAI()

app = FastAPI()


@app.get("/healthz")
def health_check():
    """Health check endpoint for monitoring service status"""
    return {"status": "ok", "service": "Relevance AI Chat Service"}


@app.post("/chat")
async def chat(req: Request):
    """Primary chat endpoint used by Node/Socket.IO backend.
    Expected JSON body:
        {
          "message": "user's prompt",
          "agent_id": "optional-agent-id",    # falls back to env AGENT_ID
          "thread_id": "optional-thread-id",  # autogenerated if missing
          "stream": true                       # default true
        }
    """
    try:
        data = await req.json()
        print(f"[DEBUG] Received request data: {data}")
        
        agent_id: str = data.get("agent_id") or os.getenv("AGENT_ID")
        if not agent_id:
            return JSONResponse({"error": "agent_id must be provided through body or env AGENT_ID"}, status_code=400)

        message: str = data.get("message")
        if not message:
            return JSONResponse({"error": "message field is required"}, status_code=400)

        thread_id: str = data.get("thread_id") or f"thread_{uuid.uuid4().hex}"
        stream: bool = bool(data.get("stream", True))
        print(f"[DEBUG] Processing request - agent_id: {agent_id}, message: {message[:20]}..., stream: {stream}")

        # Attempt streaming first
        if stream:
            try:
                def gen():
                    try:
                        for part in client.agents.chat_stream(
                            agent_id=agent_id,
                            message=message,
                            thread_id=thread_id,
                        ):
                            # Each part is assumed to be a dict with "content" key
                            chunk = part.get("content")
                            if chunk is not None:
                                yield f"data:{chunk}\n\n"
                    except Exception as inner_err:
                        print(f"[ERROR] Stream generation error: {str(inner_err)}")
                        traceback.print_exc()
                        # In case the generator errors out mid-stream, send error then stop
                        yield f"data:[STREAM_ERROR] {str(inner_err)}\n\n"
                return StreamingResponse(gen(), media_type="text/event-stream")
            except APIError as e:
                print(f"[ERROR] Relevance AI streaming error: {str(e)}")
                # Fallback only for the explicit streaming-disabled scenario
                if "stream is not enabled" not in str(e).lower():
                    raise
                # Otherwise, fall back to polling
                stream = False
                print("[INFO] Falling back to polling mode after stream error")
            except Exception as e:
                # Unknown error – force fallback
                print(f"[ERROR] Unexpected error in stream mode: {str(e)}")
                traceback.print_exc()
                stream = False

        # Fallback path → trigger task + polling
        try:
            print(f"[INFO] Using polling mode with agent_id: {agent_id}, thread_id: {thread_id}")
            task = client.agents.trigger_task(
                agent_id=agent_id,
                message=message,
                thread_id=thread_id,
            )
            print(f"[INFO] Task triggered successfully: {task.conversation_id}")
        except Exception as e:
            error_msg = f"Failed to trigger task: {str(e)}"
            print(f"[ERROR] {error_msg}")
            traceback.print_exc()
            return JSONResponse({"error": error_msg}, status_code=500)

        try:
            # Get polling timeout from environment or use default
            timeout = int(os.getenv("POLL_TIMEOUT", 30))
            print(f"[INFO] Polling for response with timeout: {timeout}s")
            
            output = client.tasks.wait_for_completion(
                task.conversation_id,
                timeout=timeout,
            )
            print(f"[INFO] Task completed successfully")
        except Exception as e:
            error_msg = f"Polling failed: {str(e)}"
            print(f"[ERROR] {error_msg}")
            traceback.print_exc()
            return JSONResponse({"error": error_msg}, status_code=500)

        # Make a best-effort to extract the assistant message from possible fields
        try:
            # Log output structure to help debug response format issues
            print(f"[DEBUG] Output type: {type(output).__name__}")
            print(f"[DEBUG] Output attributes: {dir(output)}")
            
            response_text = (
                (getattr(output, "output", {}) or {}).get("message")
                or getattr(output, "response", None)
                or getattr(output, "task", {}).get("response")
                or ""
            )
            
            print(f"[INFO] Extracted response text: {response_text[:50]}...")
            return JSONResponse({
                "thread_id": thread_id,
                "response": response_text,
            })
        except Exception as e:
            error_msg = f"Failed to extract response: {str(e)}"
            print(f"[ERROR] {error_msg}")
            traceback.print_exc()
            return JSONResponse({"error": error_msg}, status_code=500)
            
    except Exception as e:
        # Catch-all exception handler for the entire function
        error_msg = f"Unexpected error in chat endpoint: {str(e)}"
        print(f"[ERROR] {error_msg}")
        traceback.print_exc()
        return JSONResponse({"error": error_msg}, status_code=500)
